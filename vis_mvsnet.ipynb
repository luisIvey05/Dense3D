{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dense High-Quality 3D Reconstruction\n",
        "## Vis_MVSNet Approach\n",
        "\n",
        "Following the pipeline below, this tutorial demonstrates the process and steps we took to integrate Vis_MVSNet into the pipeline.\n",
        "\n",
        "![pipeline](https://github.com/luisIvey05/Dense3D/blob/main/images/pipeline.png?raw=true)\n",
        "\n",
        "\n",
        "\n",
        "The first thing to do is download the BU2 dataset located <a href=\"https://drive.google.com/file/d/1L7gdMz5H8jBdSpslS-U0i7fA--7UJCKA/view?usp=sharing\"> here </a>.\n"
      ],
      "metadata": {
        "id": "06no6DmBVlN3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSJu3dtsVjko"
      },
      "outputs": [],
      "source": [
        "! unzip -DD -q  BU2.zip -d  /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next steps are used to download COLMAP. However, due to how long it takes to calculate camera parameters and camera pose estimations it is strongly recommend to install COLMAP locally. "
      ],
      "metadata": {
        "id": "sP0SWLk1fTHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install git cmake build-essential libboost-program-options-dev libboost-filesystem-dev libboost-graph-dev libboost-regex-dev libboost-system-dev libboost-test-dev libeigen3-dev libsuitesparse-dev libfreeimage-dev libgoogle-glog-dev libgflags-dev libglew-dev qtbase5-dev libqt5opengl5-dev libcgal-dev libcgal-qt5-dev\n",
        "!sudo apt-get install libatlas-base-dev libsuitesparse-dev\n",
        "!git clone https://ceres-solver.googlesource.com/ceres-solver\n",
        "%cd ceres-solver\n",
        "!git checkout $(git describe --tags) # Checkout the latest release\n",
        "%mkdir build\n",
        "%cd build\n",
        "!cmake .. -DBUILD_TESTING=OFF -DBUILD_EXAMPLES=OFF\n",
        "!make\n",
        "!sudo make install\n",
        "!git clone https://github.com/colmap/colmap\n",
        "%cd colmap\n",
        "!git checkout dev\n",
        "%mkdir build\n",
        "%cd build\n",
        "!cmake ..\n",
        "!make\n",
        "!sudo make install\n",
        "!CC=/usr/bin/gcc-6 CXX=/usr/bin/g++-6 cmake .."
      ],
      "metadata": {
        "id": "jcmeX8WEgi7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then used COLMAP to calculate camera parameters and camera pose estimations on BU2 dataset."
      ],
      "metadata": {
        "id": "zVTpzC-3ibvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "\n",
        "!colmap feature_extractor \\\n",
        "    --database_path /content/BU2/database.db \\\n",
        "    --image_path /content/BU2/images_col\n",
        "\n",
        "!colmap exhaustive_matcher \\\n",
        "    --database_path /content/BU2/database.db\n",
        "\n",
        "!colmap mapper \\\n",
        "    --database_path /content/BU2/database.db \\\n",
        "    --image_path /content/BU2/images_col \\\n",
        "    --output_path /content/BU2/sparse_col\n",
        "\n",
        "!colmap model_converter \\\n",
        "    --input_path /content/BU2/sparse_col/0 \\\n",
        "    --output_path /content/BU2/sparse_col \\\n",
        "    --output_type TXT"
      ],
      "metadata": {
        "id": "yYzUdfIwiaVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we use Vis_MVSNet's colmap2mvsnet.py to convert the output of COLMAP to a format that Vis_MVSNet can read."
      ],
      "metadata": {
        "id": "vuFzRhdClKVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jzhangbs/Vis-MVSNet.git"
      ],
      "metadata": {
        "id": "l-Pt34Tqnv-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.4.0 numpy==1.18.1 opencv-python==4.1.2.30 tqdm==4.41.1 matplotlib==3.1.3 open3d==0.9.0.0 apex==0.1"
      ],
      "metadata": {
        "id": "XVA1ZGRdortT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Vis-MVSNet\n",
        "!python colmap2mvsnet.py --dense_folder /content/BU2 --max_d 256 --convert_format"
      ],
      "metadata": {
        "id": "0uzmknyOpkap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the next step, download our fine-tuned model <a href=\"https://drive.google.com/file/d/16vPf16wTOAV_0Z-LxV3Yc_CFi3BtSjR9/view?usp=sharing\"> here </a>. We are then going to produce the predicted depth maps."
      ],
      "metadata": {
        "id": "tv6qNsk-qMUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py --data_root /content/BU2 --dataset_name general --num_src 4 --max_d 256 --resize 1920,1088 --crop 1920,1088 --load_path /content/mid_air --write_result --result_dir /content/BU2/output"
      ],
      "metadata": {
        "id": "UIKs9haqr3Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then fuse the probabily map and depth map to get the final depth map estimation. "
      ],
      "metadata": {
        "id": "ATK8hcT8srtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python fusion.py --data /content/output --pair /content/BU2/pair.txt --vthresh 4 --pthresh .8,.7,.8"
      ],
      "metadata": {
        "id": "I1Al6a1Ds6c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to go into the \"/content/BU2/output\" directory to remove unneccsary files. \n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YoQ4Ly5AtQIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/BU2/output\n",
        "!rm -rf /*.txt\n",
        "!rm -rf /*prob.pfm\n",
        "!rm -rf /*.jpg"
      ],
      "metadata": {
        "id": "dHPsZZfU94C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then convert the pfm files to .png while scaling each pixel by 1000."
      ],
      "metadata": {
        "id": "QXEP6bWe94PE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import imageio\n",
        "import glob\n",
        "import argparse\n",
        "import shutil\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "pfm_path = \"./\"\n",
        "png_path = \"../image/\"\n",
        "print(\"[INFO] PFM_PATH {}    PNG_PATH {}   \".format(pfm_path, png_path))\n",
        "pfm_files = glob.glob(pfm_path + '/*.pfm')\n",
        "if os.path.exists(png_path):\n",
        "  shutil.rmtree(png_path)\n",
        "os.makedirs(png_path)\n",
        "total = len(pfm_files)\n",
        "for idx, fpath in enumerate(pfm_files):\n",
        "  fname = os.path.basename(fpath)\n",
        "  new_fname = os.path.splitext(fname)[0]\n",
        "  new_fname += '.png'\n",
        "  print(\"[INFO] DEPTH {} to {} ---- {} / {}\".format(fname, new_fname, idx + 1, total))\n",
        "  pfm = cv2.imread(fpath, cv2.IMREAD_UNCHANGED)\n",
        "  print(pfm.dtype)\n",
        "  pfm *= 1000\n",
        "  imageio.imwrite(png_path + new_fname, pfm.astype(np.uint16))"
      ],
      "metadata": {
        "id": "AQAjOYd3AU1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The images and the depth directory are not temporally coherent because Vis_MVSNet establishes sequences of images to eliminate occuluded pixels. However, the new arrangement of images and depth maps cannot be fed into Open3D's reconstruction system without first removing images that are out of place in time. The final B2 images and predicted depth maps can be downloaded <a href=\"https://drive.google.com/file/d/1bh2WtAF6mhRhJIA1Gp68pmicakCqar4_/view?usp=share_link\"> here </a>."
      ],
      "metadata": {
        "id": "cYnGTLAoBQN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/isl-org/Open3D.git\n",
        "%cd /content/Open3D/examples/python/reconstruction_system/\n",
        "!mkdir ./datasets\n",
        "!mkdir ./datasets/016\n",
        "!mkdir ./optional\n",
        "!pip install open3d"
      ],
      "metadata": {
        "id": "HI4i1_tABDem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please, place the images and predicted depth maps directory inside \"/content/Open3D/examples/python/reconstruction_system/datasets/016/\" and name the folders as image and depth, respectively. Once again the final B2 image and predicted depth maps can be downloaded <a href=\"https://drive.google.com/file/d/1bh2WtAF6mhRhJIA1Gp68pmicakCqar4_/view?usp=share_link\"> here </a>. You also need to download the camera instrinsic parameters found <a href=\"https://drive.google.com/file/d/1EINFlu6bJ32g8MuiH1PfWckB7nps36AO/view?usp=share_link\"> camera.json </a> and the configuration files found <a href=\"https://drive.google.com/file/d/1rhyMusfFd6gX9dAv1l5IsVM0Xd06eMaL/view?usp=share_link\">config.json</a>. The camera instrinisc parameters need to be placed inside \"/content/Open3D/examples/python/reconstruction_system/optinonal/\" and the config.json file needs to be inside \"/content/Open3D/examples/python/reconstruction_system/datasets/016/\"."
      ],
      "metadata": {
        "id": "aEXDbjmQDLsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final step is to run open3D's 3D reconstruction system. "
      ],
      "metadata": {
        "id": "SuNFnJAJE17T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_system.py --config ./datasets/016/config.json --make\n",
        "%cd ./datasets/016/fragments/"
      ],
      "metadata": {
        "id": "Rg8yLGfsDFAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The point cloud (output of the 3D reconstruction system) can be found inside /content/Open3D/examples/python/reconstruction_system/datasets/016/fragments/fragment_000.ply\"\n",
        "Use the following code below to visualize the final 3D reconstruction of the Chapel."
      ],
      "metadata": {
        "id": "ifqLfXkOFVHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import open3d as o3d\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import glob\n",
        "import copy\n",
        "from open3d_example import draw_geometries_flip\n",
        "\n",
        "flip_transform = [[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]]\n",
        "\n",
        "\n",
        "def draw_geometries_flip(pcds):\n",
        "    pcds_transform = []\n",
        "    for pcd in pcds:\n",
        "        pcd_temp = copy.deepcopy(pcd)\n",
        "        pcd_temp.transform(flip_transform)\n",
        "        pcds_transform.append(pcd_temp)\n",
        "    o3d.visualization.draw_geometries(pcds_transform)\n",
        "\n",
        "\n",
        "fragment_file = \"fragment_000.ply\"\n",
        "pcd = o3d.io.read_point_cloud(fragment_file)\n",
        "point_cloud_np = np.asarray(pcd.points)\n",
        "\n",
        "# Save the point cloud to a .npy file\n",
        "np.save(\"point_cloud.npy\", point_cloud_np)\n",
        "\n",
        "#draw_geometries_flip([pcd])\n",
        "o3d.visualization.draw_geometries([pcd])\n"
      ],
      "metadata": {
        "id": "PEMsvJm9DKhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kieOwKf_FUia"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}